# ğŸ‰ DRAGON Binary Classification - Quick Start

> **TL;DR:** Binary Classification approach Ä‘Æ¡n giáº£n hÆ¡n, nhanh hÆ¡n, vÃ  tá»± nhiÃªn hÆ¡n QA format cho Sarcasm Detection!

## ğŸ¯ Concept

Thay vÃ¬ hack DRAGON thÃ nh QA format (táº¡o 2 fake choices), chÃºng ta dÃ¹ng **Binary Classification thuáº§n tÃºy**:

```
Input Text + Graph â†’ DRAGON Encoder + GNN â†’ [CLS] â†’ Linear(1024â†’2) â†’ [Not Sarcastic, Sarcastic]
```

## âœ¨ Advantages

| Metric | Binary Classification | QA Format |
|--------|----------------------|-----------|
| **Simplicity** | âœ… Clean & simple | âŒ Hacky workaround |
| **Speed** | âœ… 1 forward pass | âŒ 2 forward passes |
| **Memory** | âœ… ~10GB | âŒ ~15GB |
| **Code length** | âœ… ~200 lines | âŒ ~500 lines |
| **Debug** | âœ… Easy | âŒ Complex |
| **Natural** | âœ… True classification | âŒ Fake QA problem |

## ğŸš€ Quick Start

### 1. Prerequisites

```bash
# Activate environment
conda activate dragon

# Ensure you have:
# âœ… Pre-trained model: models/general_model.pt
# âœ… ConceptNet preprocessed: data/cpnet/
# âœ… iSarcasm dataset: data/isarcasm/
```

### 2. One-Command Training

```bash
# Binary Classification (RECOMMENDED)
./scripts/run_train__isarcasm_binary.sh

# Takes ~1-2 hours on single GPU
# Model saved to: runs/isarcasm/dragon_binary__*/model.pt
```

### 3. Evaluation

```bash
./scripts/run_eval__isarcasm_binary.sh runs/isarcasm/dragon_binary__*/model.pt
```

## ğŸ“¦ Files Structure

```
modeling/
  â”œâ”€â”€ modeling_dragon_binary.py      â† NEW: Binary classifier wrapper
  â”œâ”€â”€ modeling_dragon.py              â† Original DRAGON
  â””â”€â”€ ...

scripts/
  â”œâ”€â”€ run_train__isarcasm_binary.sh  â† NEW: Binary training
  â”œâ”€â”€ run_eval__isarcasm_binary.sh   â† NEW: Binary evaluation
  â”œâ”€â”€ run_train__isarcasm.sh         â† OLD: QA format training
  â””â”€â”€ ...
```

## ğŸ“ How It Works

### Architecture

```python
class DRAGONBinaryClassifier(nn.Module):
    def __init__(self, ...):
        # 1. DRAGON backbone (pre-trained)
        self.dragon = DRAGON(...)
        
        # 2. Binary classification head (random init)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(1024, 2)  # [Not Sarcastic, Sarcastic]
    
    def forward(self, inputs):
        # Get [CLS] representation from DRAGON
        pooled = self.dragon(inputs)  # [batch, 1024]
        
        # Classify
        logits = self.classifier(self.dropout(pooled))  # [batch, 2]
        return logits
```

### Training Strategy

```python
# 1. Load pre-trained DRAGON
model = DRAGONBinaryClassifier(...)
model.load_pretrained_dragon('models/general_model.pt')

# 2. Different learning rates
optimizer = AdamW([
    {'params': model.dragon.encoder.parameters(), 'lr': 2e-5},  # Pre-trained
    {'params': model.dragon.gnn.parameters(), 'lr': 1e-3},      # Pre-trained
    {'params': model.classifier.parameters(), 'lr': 1e-3}       # Random init
])

# 3. Fine-tune 10 epochs
for epoch in range(10):
    for batch in dataloader:
        logits = model(batch)  # [batch, 2]
        loss = F.cross_entropy(logits, labels)
        loss.backward()
        optimizer.step()
```

## ğŸ” Code Examples

### Import & Initialize

```python
from modeling.modeling_dragon_binary import (
    DRAGONBinaryClassifier, 
    create_optimizer_grouped_parameters
)

# Initialize
model = DRAGONBinaryClassifier(
    args=args,
    k=5,              # 5 GNN layers
    n_ntype=4,        # Node types
    n_etype=38,       # Edge types
    sent_dim=1024,    # RoBERTa-large
    n_concept=799273, # ConceptNet
    concept_dim=200,
    concept_in_dim=200,
    hidden_size=1024,
    dropout=0.1
)

# Load pre-trained weights
model.load_pretrained_dragon('models/general_model.pt')
# âœ… Encoder + GNN loaded from checkpoint
# âœ… Classifier initialized randomly (will fine-tune)
```

### Training Loop

```python
# Setup optimizer
param_groups = create_optimizer_grouped_parameters(model, args)
optimizer = AdamW(param_groups)

# Training
for epoch in range(10):
    model.train()
    for batch in train_loader:
        # Unpack
        input_ids, attention_mask, concept_ids, node_types, adj, labels = batch
        
        # Forward
        logits = model(input_ids, attention_mask, concept_ids, node_types, adj)
        
        # Loss
        loss = F.cross_entropy(logits, labels)
        
        # Backward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    
    # Evaluate
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in dev_loader:
            logits = model(*batch[:-1])
            preds = logits.argmax(dim=-1)
            correct += (preds == batch[-1]).sum().item()
            total += batch[-1].size(0)
    
    acc = correct / total
    print(f"Epoch {epoch}: Dev Acc = {acc:.4f}")
```

### Inference

```python
# Load trained model
model = DRAGONBinaryClassifier.load('runs/isarcasm/dragon_binary__*/model.pt')
model.eval()

# Predict
text = "Great weather for a walk! â˜”"  # Sarcastic (raining)
inputs = preprocess(text, knowledge_graph)

with torch.no_grad():
    logits = model(**inputs)
    probs = F.softmax(logits, dim=-1)
    pred = logits.argmax(dim=-1)

print(f"Prediction: {'Sarcastic' if pred == 1 else 'Not Sarcastic'}")
print(f"Confidence: {probs[0, pred]:.2%}")
```

## ğŸ“Š Expected Results

| Model | Accuracy | Training Time | Memory |
|-------|----------|---------------|--------|
| **DRAGON Binary** | **~87-90%** | 1-2 hours | 10GB |
| DRAGON QA Format | ~85-88% | 2-3 hours | 15GB |
| BERT-base | ~75-80% | 1 hour | 8GB |
| Rule-based | ~65% | - | - |

## ğŸ› Troubleshooting

### "RuntimeError: CUDA out of memory"
```bash
# Reduce batch size
bs=16  # in run_train__isarcasm_binary.sh
mbs=1
```

### "No improvement after 10 epochs"
```bash
# Check learning rates
elr="1e-5"  # Try lower LR
dlr="5e-4"
```

### "Model predicts all one class"
```python
# Check data balance
from collections import Counter
labels = [data['label'] for data in dataset]
print(Counter(labels))  # Should be ~50-50

# Check loss function
# Should be CrossEntropyLoss for 2 classes
loss = F.cross_entropy(logits, labels)  # âœ… Correct
```

## ğŸ“š Reference

### Papers
- **DRAGON**: [EMNLP 2022](https://arxiv.org/abs/2210.09338) - Yasunaga et al.
- **iSarcasm**: Task A English - Sarcasm Detection

### Files
- **Model Wrapper**: [modeling/modeling_dragon_binary.py](modeling/modeling_dragon_binary.py)
- **Training Script**: [scripts/run_train__isarcasm_binary.sh](scripts/run_train__isarcasm_binary.sh)
- **Full Guide**: [FINE_TUNING_GUIDE.md](FINE_TUNING_GUIDE.md)

## ğŸ’¡ Key Insights

1. **DRAGON = Encoder** (giá»‘ng BERT), nÃªn lÃ m Classification ráº¥t tá»± nhiÃªn
2. **QA format** lÃ  cÃ¡ch hack, khÃ´ng pháº£i cÃ¡ch tá»‘i Æ°u
3. **Binary Classification** Ä‘Æ¡n giáº£n, nhanh, vÃ  hiá»‡u quáº£ hÆ¡n
4. **Pre-trained reasoning** cá»§a DRAGON lá»£i háº¡i cho Sarcasm (incongruity detection)

## ğŸ‰ Next Steps

1. âœ… Train model vá»›i binary approach
2. âœ… Compare vá»›i QA format baseline
3. âœ… Analyze error cases
4. ğŸ’¡ Try ensemble vá»›i BERT?
5. ğŸ’¡ Experiment vá»›i different GNN layers?

---

**Inspired by**: Gemini 3 Pro's wisdom ğŸ§ 

**Motto**: "ÄÆ¡n giáº£n lÃ  tá»‘t nháº¥t. Phá»©c táº¡p thÃ¬ khá»•!" ğŸ˜
