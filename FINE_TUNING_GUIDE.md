# ðŸ‰ DRAGON Fine-tuning cho Sarcasm Detection

## âš ï¸ QUAN TRá»ŒNG: KhÃ´ng Train From Scratch!

**Báº®T BUá»˜C pháº£i dÃ¹ng Fine-tuning approach:**

### Táº¡i sao KHÃ”NG nÃªn train from scratch?

1. **Tá»‘n kÃ©m khá»§ng khiáº¿p**: Train DRAGON tá»« Ä‘áº§u cáº§n:
   - Cá»¥m GPU máº¡nh (multi-GPU cluster)
   - HÃ ng ngÃ y/tuáº§n training
   - Dataset khá»•ng lá»“ (BookCorpus + Wikipedia + ConceptNet)
   - Chi phÃ­ hÃ ng ngÃ n Ä‘Ã´ la

2. **LÃ£ng phÃ­ tri thá»©c**: 
   - Pre-trained DRAGON Ä‘Ã£ há»c Ä‘Æ°á»£c reasoning vÃ  quan há»‡ tá»« ConceptNet
   - VÃ­ dá»¥: `rain â†’ causes â†’ wet`, `sunny â†’ antonym â†’ rainy`
   - Dataset iSarcasm (~4000 samples) quÃ¡ nhá» Ä‘á»ƒ há»c láº¡i tá»« Ä‘áº§u

3. **Káº¿t quáº£ kÃ©m hÆ¡n**:
   - Train from scratch vá»›i data Ã­t = model "ngu"
   - Fine-tuning = káº¿ thá»«a tri thá»©c + há»c skill má»›i = hiá»‡u quáº£

## ðŸŽ¯ Chiáº¿n thuáº­t: Fine-tuning Pre-trained Model

```
Pre-trained DRAGON (Ä‘Ã£ há»c ConceptNet)
           â†“
    Load weights
           â†“
  Fine-tune 3-5 epochs trÃªn iSarcasm
           â†“
    Model cho Sarcasm Detection
```

## ðŸš€ Quick Start

### 1. Chuáº©n bá»‹

```bash
# Activate environment
conda activate dragon
cd /home/hqvjet/Projects/dragon

# Download pre-trained model (náº¿u chÆ°a cÃ³)
mkdir -p models
cd models
wget https://nlp.stanford.edu/projects/myasu/DRAGON/models/general_model.pt
cd ..

# Ensure ConceptNet is ready
python preprocess.py --run common -p 8
```

### 2. Download vÃ  Preprocess iSarcasm Data

```bash
# Tá»± Ä‘á»™ng download tá»« HuggingFace vÃ  preprocess
python preprocess.py --run isarcasm -p 8
```

Lá»‡nh nÃ y sáº½:
- âœ… Download dataset tá»« HuggingFace: `viethq1906/isarcasm_2022_taskA_En`
- âœ… Convert sang format DRAGON (2-choice: sarcastic/not sarcastic)
- âœ… Ground concepts vá»›i ConceptNet
- âœ… Extract subgraphs
- âœ… Táº¡o graph adjacency data

### 3. Fine-tune Model

```bash
chmod +x scripts/run_train__isarcasm.sh
./scripts/run_train__isarcasm.sh
```

**QuÃ¡ trÃ¬nh fine-tuning:**
- ðŸ”¥ Load `general_model.pt` (pre-trained weights)
- ðŸŽ¯ Unfreeze táº¥t cáº£ parameters ngay tá»« epoch 0
- ðŸ“š Train 10 epochs (thay vÃ¬ 15-20 nhÆ° train from scratch)
- ðŸ§  Model há»c nháº­n diá»‡n sarcasm dá»±a trÃªn incongruity (mÃ¢u thuáº«n)
- ðŸ’¾ Save best checkpoint dá»±a trÃªn dev accuracy

## ðŸ” Táº¡i sao DRAGON máº¡nh cho Sarcasm?

### Sarcasm = Incongruity (MÃ¢u thuáº«n)

**VÃ­ dá»¥:**
> "Trá»i mÆ°a táº§m tÃ£, thá»i tiáº¿t Ä‘áº¹p tuyá»‡t vá»i!" â˜”â†’â˜€ï¸ 

### DRAGON phÃ¡t hiá»‡n mÃ¢u thuáº«n qua ConceptNet:

```
rain --[Causes]--> wet
rain --[Antonym]--> sunny
sunny --[RelatedTo]--> nice weather

âŒ PhÃ¡t hiá»‡n: "rain" vÃ  "nice weather" mÃ¢u thuáº«n
âœ… Káº¿t luáº­n: Sarcasm!
```

### CÃ¡c relation quan trá»ng cho Sarcasm:

- `Antonym`: Tá»« trÃ¡i nghÄ©a (rain â†” sunny)
- `DistinctFrom`: KhÃ¡c biá»‡t rÃµ rÃ ng
- `Causes`: Quan há»‡ nhÃ¢n quáº£
- `NotDesires`: KhÃ´ng mong muá»‘n

**âš ï¸ LÆ¯U Ã:** Khi preprocess, KHÃ”NG filter bá» cÃ¡c edge `Antonym` vÃ  `DistinctFrom` - Ä‘Ã¢y lÃ  chÃ¬a khÃ³a cho sarcasm detection!

## ðŸ“Š Hyperparameters cho Fine-tuning

### Tá»‘i Æ°u cho iSarcasm:

```bash
# Learning rates (higher than feature extraction)
elr=2e-5              # Encoder LR (fine-tuning)
dlr=1e-3              # Decoder (GNN) LR

# Training config
bs=32                 # Batch size (smaller for stability)
n_epochs=10           # Fewer epochs (fine-tuning converges fast)
unfreeze_epoch=0      # Unfreeze immediately
warmup_steps=150      # More warmup for stability

# Model config
k=5                   # 5 GNN layers
gnndim=200            # GNN dimension
max_seq_len=128       # Sarcasm text usually short

# Reproducibility
seed=42               # Fixed seed
# cudnn.deterministic=True (auto-enabled)
```

### So sÃ¡nh vá»›i Training from Scratch:

| Metric | Fine-tuning | From Scratch |
|--------|-------------|--------------|
| Epochs | 10 | 50+ |
| Learning Rate | 2e-5 (encoder) | 1e-5 |
| Unfreeze Epoch | 0 (immediate) | 2-3 |
| Data Required | 4K samples OK | 100K+ samples |
| Training Time | 1-2 hours | Days/Weeks |
| GPU Memory | 10GB | 16GB+ |
| Final Accuracy | 85-90% | 70-75% (vá»›i data Ã­t) |

## ðŸŽ“ Kiáº¿n trÃºc Model

### ðŸ†• Binary Classification Approach (RECOMMENDED) â­

**Insight tá»« Gemini Pro:** DRAGON Ä‘Æ°á»£c thiáº¿t káº¿ cho QA, nhÆ°ng báº£n cháº¥t lÃ  **Encoder** nÃªn hoÃ n toÃ n lÃ m Ä‘Æ°á»£c Classification thuáº§n tÃºy - Ä‘Æ¡n giáº£n hÆ¡n, nhanh hÆ¡n, tá»± nhiÃªn hÆ¡n!

```python
Input Text + Graph
       â†“
RoBERTa Encoder (pre-trained)
       â†“
Information Exchange Layers
       â†“
GNN (pre-trained on ConceptNet)
       â†“
Pooling ([CLS] representation)
       â†“
Dropout (0.1)
       â†“
Linear Layer (1024 -> 2)
       â†“
Softmax
       â†“
[Not Sarcastic, Sarcastic]
```

**Æ¯u Ä‘iá»ƒm so vá»›i QA format:**
- âœ… **ÄÆ¡n giáº£n hÆ¡n:** KhÃ´ng cáº§n táº¡o fake choices (A/B)
- âœ… **Nhanh hÆ¡n:** Chá»‰ 1 forward pass thay vÃ¬ 2
- âœ… **Tá»± nhiÃªn hÆ¡n:** ÄÃºng báº£n cháº¥t cá»§a classification
- âœ… **Ãt memory hÆ¡n:** KhÃ´ng pháº£i duplicate input
- âœ… **Dá»… debug hÆ¡n:** Code ngáº¯n gá»n, clear hÆ¡n

**Implementation:** ÄÃ£ táº¡o sáºµn wrapper táº¡i [`modeling/modeling_dragon_binary.py`](modeling/modeling_dragon_binary.py)

### Alternative: QA Format Approach

```python
# CÃ¡ch cÅ© (váº«n work nhÆ°ng phá»©c táº¡p hÆ¡n):
# Convert iSarcasm thÃ nh 2-choice format
Question: "Tweet text here"
Choices:
  A: "This text is sarcastic"      â† Answer if label=1
  B: "This text is not sarcastic"  â† Answer if label=0

# â†’ Model cháº¡y 2 láº§n (cho A vÃ  B), chá»n score cao hÆ¡n
```

**Káº¿t luáº­n:** DÃ¹ng Binary Classification cho Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£. Chá»‰ dÃ¹ng QA format náº¿u báº¡n muá»‘n test kháº£ nÄƒng reasoning phá»©c táº¡p hÆ¡n.

## ðŸ“ˆ Evaluation

```bash
# Evaluate trÃªn test set
./scripts/run_eval__isarcasm.sh runs/isarcasm/YOUR_RUN_NAME/model.pt
```

## ðŸ”¬ Advanced: Understanding the Code

### ðŸ†• Binary Classification Implementation

**File má»›i:** [`modeling/modeling_dragon_binary.py`](modeling/modeling_dragon_binary.py)

```python
# 1. Import wrapper
from modeling.modeling_dragon_binary import (
    DRAGONBinaryClassifier, 
    create_optimizer_grouped_parameters
)

# 2. Initialize model
model = DRAGONBinaryClassifier(
    args=args,
    k=5,                    # 5 GNN layers
    n_ntype=4,              # 4 node types
    n_etype=38,             # 38 edge types
    sent_dim=1024,          # RoBERTa-large hidden size
    n_concept=799273,       # ConceptNet concepts
    concept_dim=200,
    concept_in_dim=200,
    hidden_size=1024,
    dropout=0.1
)

# 3. Load pre-trained DRAGON weights
model.load_pretrained_dragon('models/general_model.pt')
# â†’ Encoder + GNN Ä‘Æ°á»£c load
# â†’ Binary classifier head khá»Ÿi táº¡o random (sáº½ Ä‘Æ°á»£c fine-tune)

# 4. Setup optimizer vá»›i grouped learning rates
param_groups = create_optimizer_grouped_parameters(model, args)
optimizer = AdamW(param_groups)
# â†’ Encoder: 2e-5 (pre-trained, cáº§n LR tháº¥p)
# â†’ GNN: 1e-3 (pre-trained nhÆ°ng cáº§n adapt)
# â†’ Classifier: 1e-3 (random init, cáº§n LR cao)

# 5. Training loop
for epoch in range(10):
    for batch in dataloader:
        # Unpack batch
        input_ids, attention_mask, concept_ids, node_types, adj, labels = batch
        
        # Forward pass
        logits = model(input_ids, attention_mask, concept_ids, node_types, adj)
        # logits shape: [batch_size, 2]
        
        # Compute loss
        loss = F.cross_entropy(logits, labels)
        
        # Backward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

**So sÃ¡nh vá»›i QA Format:**

| Aspect | Binary Classification | QA Format |
|--------|----------------------|-----------|
| Code complexity | âœ… ÄÆ¡n giáº£n (~200 lines) | âŒ Phá»©c táº¡p (~500 lines) |
| Forward passes | âœ… 1 láº§n | âŒ 2 láº§n (cho má»—i choice) |
| Memory usage | âœ… 10GB | âŒ 15GB |
| Training speed | âœ… 1x | âŒ 0.5x (cháº­m hÆ¡n 2x) |
| Debug difficulty | âœ… Dá»… | âŒ KhÃ³ |

### Data Flow trong Fine-tuning (OLD - QA Format):

```python
# CÃCH CÅ¨ - Giá»¯ láº¡i Ä‘á»ƒ tham kháº£o
# 1. Load pre-trained DRAGON
model = DRAGON(args, ...)
state_dict = torch.load('general_model.pt')
model.load_state_dict(state_dict)

# 2. Setup optimizer vá»›i learning rates khÃ¡c nhau
encoder_params = [p for n, p in model.named_parameters() if 'encoder' in n]
decoder_params = [p for n, p in model.named_parameters() if 'gnn' in n]

optimizer = AdamW([
    {'params': encoder_params, 'lr': 2e-5},  # Lower LR cho pre-trained parts
    {'params': decoder_params, 'lr': 1e-3}   # Higher LR cho GNN
])

# 3. Fine-tune
for epoch in range(10):  # Ãt epochs
    for batch in dataloader:
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

### Key Modifications:

1. **ðŸ†• modeling/modeling_dragon_binary.py** (NEW FILE):
   - âœ… `DRAGONBinaryClassifier`: Wrapper thÃªm Linear head (1024â†’2)
   - âœ… `load_pretrained_dragon()`: Load pre-trained weights
   - âœ… `create_optimizer_grouped_parameters()`: Setup grouped LRs
   - âœ… Clean, simple, efficient

2. **dragon.py**: 
   - âœ… Enhanced seed setting
   - âœ… CuDNN deterministic mode
   - âš ï¸ NO changes to model logic (hoáº·c sáº½ update Ä‘á»ƒ support binary mode)

3. **preprocess_utils/convert_isarcasm.py**:
   - âœ… Download tá»« HuggingFace
   - âœ… Convert binary labels (0/1)
   - âš ï¸ OLD: Convert sang 2-choice QA format (khÃ´ng cáº§n ná»¯a vá»›i binary approach)

4. **ðŸ†• scripts/run_train__isarcasm_binary.sh** (TODO):
   - âœ… Use `DRAGONBinaryClassifier` thay vÃ¬ `DRAGON`
   - âœ… Simpler hyperparameters
   - âœ… Faster training

## âš¡ Performance Tips

### 1. Use Mixed Precision (FP16):
```bash
fp16=true  # Already enabled in script
# â†’ 2x faster training, 50% less memory
```

### 2. Gradient Accumulation:
```bash
bs=32       # Physical batch size
mbs=2       # Mini-batch size
# â†’ Effective batch size = 32, but process 2 at a time
# â†’ Fits in smaller GPUs
```

### 3. Early Stopping:
```bash
max_epochs_before_stop=5
# â†’ Stop if no improvement for 5 epochs
# â†’ Prevent overfitting
```

## ðŸ› Troubleshooting

### "RuntimeError: CUDA out of memory"
```bash
# Solution 1: Reduce batch size
bs=16
mbs=1

# Solution 2: Use CPU for some operations
# (already handled in code)
```

### "No improvement after 10 epochs"
```bash
# Possible causes:
# 1. Learning rate quÃ¡ cao/tháº¥p
#    â†’ Try elr=1e-5 or elr=3e-5

# 2. Data imbalance
#    â†’ Check label distribution

# 3. Need more data augmentation
#    â†’ Consider back-translation, paraphrasing
```

### "Model predicts all one class"
```bash
# Check:
1. Data balance: ~50-50 distribution?
2. Loss function: CrossEntropyLoss cho 2 classes
3. Metrics: Accuracy, F1, Precision, Recall
```

## ðŸ“š References

### Papers:
- **DRAGON**: [NeurIPS 2022](https://arxiv.org/abs/2210.09338)
- **iSarcasm**: Dataset paper (2022)

### Pre-trained Models:
- Download: https://nlp.stanford.edu/projects/myasu/DRAGON/models/general_model.pt
- Size: 360M parameters
- Domain: General (ConceptNet + BookCorpus)

## ðŸŽ¯ Expected Results

### Baseline (Rule-based):
- Accuracy: ~65%

### BERT-base (fine-tuned):
- Accuracy: ~75-80%

### DRAGON (fine-tuned):
- **Expected: 85-90%** âœ¨
- Improvement: +5-10% over BERT
- Why: Graph reasoning cho incongruity detection

## âœ… Checklist

TrÆ°á»›c khi train:
- [ ] Downloaded `general_model.pt`
- [ ] Preprocessed ConceptNet (`python preprocess.py --run common`)
- [ ] Downloaded iSarcasm (`python preprocess.py --run isarcasm`)
- [ ] Checked GPU memory (>=10GB free)
- [ ] Set `load_model_path=models/general_model.pt` in script

Trong khi train:
- [ ] Monitor loss (should decrease steadily)
- [ ] Check dev accuracy every epoch
- [ ] Watch for overfitting (train acc >> dev acc)
- [ ] Logs saved to `logs/train__*.log.txt`

Sau khi train:
- [ ] Best model saved to `runs/isarcasm/*/model.pt`
- [ ] Evaluate on test set
- [ ] Compare vá»›i baseline
- [ ] Analyze error cases

---

**ðŸŽ‰ Good luck with fine-tuning! Remember: Pre-trained > From Scratch!**
