# ðŸ‰ DRAGON Fine-tuning cho Sarcasm Detection

## âš ï¸ QUAN TRá»ŒNG: KhÃ´ng Train From Scratch!

**Báº®T BUá»˜C pháº£i dÃ¹ng Fine-tuning approach:**

### Táº¡i sao KHÃ”NG nÃªn train from scratch?

1. **Tá»‘n kÃ©m khá»§ng khiáº¿p**: Train DRAGON tá»« Ä‘áº§u cáº§n:
   - Cá»¥m GPU máº¡nh (multi-GPU cluster)
   - HÃ ng ngÃ y/tuáº§n training
   - Dataset khá»•ng lá»“ (BookCorpus + Wikipedia + ConceptNet)
   - Chi phÃ­ hÃ ng ngÃ n Ä‘Ã´ la

2. **LÃ£ng phÃ­ tri thá»©c**: 
   - Pre-trained DRAGON Ä‘Ã£ há»c Ä‘Æ°á»£c reasoning vÃ  quan há»‡ tá»« ConceptNet
   - VÃ­ dá»¥: `rain â†’ causes â†’ wet`, `sunny â†’ antonym â†’ rainy`
   - Dataset iSarcasm (~4000 samples) quÃ¡ nhá» Ä‘á»ƒ há»c láº¡i tá»« Ä‘áº§u

3. **Káº¿t quáº£ kÃ©m hÆ¡n**:
   - Train from scratch vá»›i data Ã­t = model "ngu"
   - Fine-tuning = káº¿ thá»«a tri thá»©c + há»c skill má»›i = hiá»‡u quáº£

## ðŸŽ¯ Chiáº¿n thuáº­t: Fine-tuning Pre-trained Model

```
Pre-trained DRAGON (Ä‘Ã£ há»c ConceptNet)
           â†“
    Load weights
           â†“
  Fine-tune 3-5 epochs trÃªn iSarcasm
           â†“
    Model cho Sarcasm Detection
```

## ðŸš€ Quick Start

### 1. Chuáº©n bá»‹

```bash
# Activate environment
conda activate dragon
cd /home/hqvjet/Projects/dragon

# Download pre-trained model (náº¿u chÆ°a cÃ³)
mkdir -p models
cd models
wget https://nlp.stanford.edu/projects/myasu/DRAGON/models/general_model.pt
cd ..

# Ensure ConceptNet is ready
python preprocess.py --run common -p 8
```

### 2. Download vÃ  Preprocess iSarcasm Data

```bash
# Tá»± Ä‘á»™ng download tá»« HuggingFace vÃ  preprocess
python preprocess.py --run isarcasm -p 8
```

Lá»‡nh nÃ y sáº½:
- âœ… Download dataset tá»« HuggingFace: `viethq1906/isarcasm_2022_taskA_En`
- âœ… Convert sang format DRAGON (2-choice: sarcastic/not sarcastic)
- âœ… Ground concepts vá»›i ConceptNet
- âœ… Extract subgraphs
- âœ… Táº¡o graph adjacency data

### 3. Fine-tune Model

```bash
chmod +x scripts/run_train__isarcasm.sh
./scripts/run_train__isarcasm.sh
```

**QuÃ¡ trÃ¬nh fine-tuning:**
- ðŸ”¥ Load `general_model.pt` (pre-trained weights)
- ðŸŽ¯ Unfreeze táº¥t cáº£ parameters ngay tá»« epoch 0
- ðŸ“š Train 10 epochs (thay vÃ¬ 15-20 nhÆ° train from scratch)
- ðŸ§  Model há»c nháº­n diá»‡n sarcasm dá»±a trÃªn incongruity (mÃ¢u thuáº«n)
- ðŸ’¾ Save best checkpoint dá»±a trÃªn dev accuracy

## ðŸ” Táº¡i sao DRAGON máº¡nh cho Sarcasm?

### Sarcasm = Incongruity (MÃ¢u thuáº«n)

**VÃ­ dá»¥:**
> "Trá»i mÆ°a táº§m tÃ£, thá»i tiáº¿t Ä‘áº¹p tuyá»‡t vá»i!" â˜”â†’â˜€ï¸ 

### DRAGON phÃ¡t hiá»‡n mÃ¢u thuáº«n qua ConceptNet:

```
rain --[Causes]--> wet
rain --[Antonym]--> sunny
sunny --[RelatedTo]--> nice weather

âŒ PhÃ¡t hiá»‡n: "rain" vÃ  "nice weather" mÃ¢u thuáº«n
âœ… Káº¿t luáº­n: Sarcasm!
```

### CÃ¡c relation quan trá»ng cho Sarcasm:

- `Antonym`: Tá»« trÃ¡i nghÄ©a (rain â†” sunny)
- `DistinctFrom`: KhÃ¡c biá»‡t rÃµ rÃ ng
- `Causes`: Quan há»‡ nhÃ¢n quáº£
- `NotDesires`: KhÃ´ng mong muá»‘n

**âš ï¸ LÆ¯U Ã:** Khi preprocess, KHÃ”NG filter bá» cÃ¡c edge `Antonym` vÃ  `DistinctFrom` - Ä‘Ã¢y lÃ  chÃ¬a khÃ³a cho sarcasm detection!

## ðŸ“Š Hyperparameters cho Fine-tuning

### Tá»‘i Æ°u cho iSarcasm:

```bash
# Learning rates (higher than feature extraction)
elr=2e-5              # Encoder LR (fine-tuning)
dlr=1e-3              # Decoder (GNN) LR

# Training config
bs=32                 # Batch size (smaller for stability)
n_epochs=10           # Fewer epochs (fine-tuning converges fast)
unfreeze_epoch=0      # Unfreeze immediately
warmup_steps=150      # More warmup for stability

# Model config
k=5                   # 5 GNN layers
gnndim=200            # GNN dimension
max_seq_len=128       # Sarcasm text usually short

# Reproducibility
seed=42               # Fixed seed
# cudnn.deterministic=True (auto-enabled)
```

### So sÃ¡nh vá»›i Training from Scratch:

| Metric | Fine-tuning | From Scratch |
|--------|-------------|--------------|
| Epochs | 10 | 50+ |
| Learning Rate | 2e-5 (encoder) | 1e-5 |
| Unfreeze Epoch | 0 (immediate) | 2-3 |
| Data Required | 4K samples OK | 100K+ samples |
| Training Time | 1-2 hours | Days/Weeks |
| GPU Memory | 10GB | 16GB+ |
| Final Accuracy | 85-90% | 70-75% (vá»›i data Ã­t) |

## ðŸŽ“ Kiáº¿n trÃºc Model

### Pre-trained DRAGON:

```python
Input Text + Graph
       â†“
RoBERTa Encoder (pre-trained on text)
       â†“
Information Exchange Layers
       â†“
GNN (pre-trained on ConceptNet)
       â†“
Classification Head (2 classes)
       â†“
[Sarcastic, Not Sarcastic]
```

### Fine-tuning Strategy:

```python
# Load pre-trained body
dragon = load_pretrained('general_model.pt')

# Original classification head (5 choices for CSQA)
# â†’ Replace with binary head (2 choices for sarcasm)

# Tuy nhiÃªn, cÃ¡ch dá»… nháº¥t:
# Convert iSarcasm thÃ nh 2-choice format
# â†’ DÃ¹ng luÃ´n DRAGON architecture hiá»‡n táº¡i!

Question: "Tweet text here"
Choices:
  A: "This text is sarcastic"      â† Answer if label=1
  B: "This text is not sarcastic"  â† Answer if label=0
```

## ðŸ“ˆ Evaluation

```bash
# Evaluate trÃªn test set
./scripts/run_eval__isarcasm.sh runs/isarcasm/YOUR_RUN_NAME/model.pt
```

## ðŸ”¬ Advanced: Understanding the Code

### Data Flow trong Fine-tuning:

```python
# 1. Load pre-trained DRAGON
model = DRAGON(args, ...)
state_dict = torch.load('general_model.pt')
model.load_state_dict(state_dict)

# 2. Setup optimizer vá»›i learning rates khÃ¡c nhau
encoder_params = [p for n, p in model.named_parameters() if 'encoder' in n]
decoder_params = [p for n, p in model.named_parameters() if 'gnn' in n]

optimizer = AdamW([
    {'params': encoder_params, 'lr': 2e-5},  # Lower LR cho pre-trained parts
    {'params': decoder_params, 'lr': 1e-3}   # Higher LR cho GNN
])

# 3. Fine-tune
for epoch in range(10):  # Ãt epochs
    for batch in dataloader:
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

### Key Modifications:

1. **dragon.py**: 
   - âœ… Enhanced seed setting
   - âœ… CuDNN deterministic mode
   - âš ï¸ NO changes to model logic

2. **preprocess_utils/convert_isarcasm.py**:
   - âœ… Download tá»« HuggingFace
   - âœ… Convert binary â†’ 2-choice format
   - âœ… Compatible vá»›i DRAGON architecture

3. **scripts/run_train__isarcasm.sh**:
   - âœ… Optimized hyperparameters cho fine-tuning
   - âœ… Load `general_model.pt` báº¯t buá»™c
   - âœ… Save model Ä‘á»ƒ reuse

## âš¡ Performance Tips

### 1. Use Mixed Precision (FP16):
```bash
fp16=true  # Already enabled in script
# â†’ 2x faster training, 50% less memory
```

### 2. Gradient Accumulation:
```bash
bs=32       # Physical batch size
mbs=2       # Mini-batch size
# â†’ Effective batch size = 32, but process 2 at a time
# â†’ Fits in smaller GPUs
```

### 3. Early Stopping:
```bash
max_epochs_before_stop=5
# â†’ Stop if no improvement for 5 epochs
# â†’ Prevent overfitting
```

## ðŸ› Troubleshooting

### "RuntimeError: CUDA out of memory"
```bash
# Solution 1: Reduce batch size
bs=16
mbs=1

# Solution 2: Use CPU for some operations
# (already handled in code)
```

### "No improvement after 10 epochs"
```bash
# Possible causes:
# 1. Learning rate quÃ¡ cao/tháº¥p
#    â†’ Try elr=1e-5 or elr=3e-5

# 2. Data imbalance
#    â†’ Check label distribution

# 3. Need more data augmentation
#    â†’ Consider back-translation, paraphrasing
```

### "Model predicts all one class"
```bash
# Check:
1. Data balance: ~50-50 distribution?
2. Loss function: CrossEntropyLoss cho 2 classes
3. Metrics: Accuracy, F1, Precision, Recall
```

## ðŸ“š References

### Papers:
- **DRAGON**: [NeurIPS 2022](https://arxiv.org/abs/2210.09338)
- **iSarcasm**: Dataset paper (2022)

### Pre-trained Models:
- Download: https://nlp.stanford.edu/projects/myasu/DRAGON/models/general_model.pt
- Size: 360M parameters
- Domain: General (ConceptNet + BookCorpus)

## ðŸŽ¯ Expected Results

### Baseline (Rule-based):
- Accuracy: ~65%

### BERT-base (fine-tuned):
- Accuracy: ~75-80%

### DRAGON (fine-tuned):
- **Expected: 85-90%** âœ¨
- Improvement: +5-10% over BERT
- Why: Graph reasoning cho incongruity detection

## âœ… Checklist

TrÆ°á»›c khi train:
- [ ] Downloaded `general_model.pt`
- [ ] Preprocessed ConceptNet (`python preprocess.py --run common`)
- [ ] Downloaded iSarcasm (`python preprocess.py --run isarcasm`)
- [ ] Checked GPU memory (>=10GB free)
- [ ] Set `load_model_path=models/general_model.pt` in script

Trong khi train:
- [ ] Monitor loss (should decrease steadily)
- [ ] Check dev accuracy every epoch
- [ ] Watch for overfitting (train acc >> dev acc)
- [ ] Logs saved to `logs/train__*.log.txt`

Sau khi train:
- [ ] Best model saved to `runs/isarcasm/*/model.pt`
- [ ] Evaluate on test set
- [ ] Compare vá»›i baseline
- [ ] Analyze error cases

---

**ðŸŽ‰ Good luck with fine-tuning! Remember: Pre-trained > From Scratch!**
