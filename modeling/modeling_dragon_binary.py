"""
üêâ DRAGON Binary Classification Wrapper
========================================

Thi·∫øt k·∫ø cho Sarcasm Detection v√† c√°c b√†i to√°n Binary Classification kh√°c.

Thay v√¨ d√πng QA format ph·ª©c t·∫°p (t·∫°o 2 choices gi·∫£ t·∫°o), 
wrapper n√†y ƒë∆°n gi·∫£n th√™m m·ªôt Linear Layer l√™n top c·ªßa DRAGON encoder.

Architecture:
    Input Text + Graph 
        ‚Üì
    RoBERTa Encoder (from DRAGON)
        ‚Üì
    Information Exchange Layers
        ‚Üì
    GNN (from DRAGON)
        ‚Üì
    Pooling ([CLS] token representation)
        ‚Üì
    Dropout
        ‚Üì
    Linear Layer (hidden_dim -> 2)
        ‚Üì
    [Not Sarcastic, Sarcastic]

Advantages over QA format:
    ‚úÖ ƒê∆°n gi·∫£n h∆°n - kh√¥ng c·∫ßn t·∫°o fake choices
    ‚úÖ Nhanh h∆°n - ch·ªâ 1 forward pass thay v√¨ 2
    ‚úÖ T·ª± nhi√™n h∆°n - ƒë√∫ng b·∫£n ch·∫•t c·ªßa classification
    ‚úÖ √çt memory h∆°n - kh√¥ng ph·∫£i duplicate input

Author: Adapted from DRAGON (Yasunaga et al., 2022)
"""

import torch
import torch.nn as nn
from modeling.modeling_dragon import DRAGON
from transformers import AutoModel, AutoTokenizer


class DRAGONBinaryClassifier(nn.Module):
    """
    Binary Classification wrapper cho DRAGON.
    
    Args:
        args: Arguments object v·ªõi c√°c config c·∫ßn thi·∫øt
        k: S·ªë GNN layers (default: 5)
        n_ntype: S·ªë node types trong graph (default: 4)
        n_etype: S·ªë edge types trong graph (default: 38)
        sent_dim: Dimension c·ªßa sentence embedding t·ª´ encoder
        n_concept: S·ªë concepts trong knowledge graph
        concept_dim: Dimension c·ªßa concept embeddings (default: 200)
        concept_in_dim: Input dimension cho concepts (default: 200)
        hidden_size: Hidden size c·ªßa encoder (default: 1024 cho RoBERTa-large)
        pretrained_concept_emb: Pre-trained concept embeddings
        freeze_ent_emb: C√≥ freeze concept embeddings kh√¥ng (default: True)
        init_range: Range ƒë·ªÉ init weights
        dropout: Dropout rate (default: 0.1)
    """
    
    def __init__(self, args, k, n_ntype, n_etype, sent_dim,
                 n_concept, concept_dim, concept_in_dim, hidden_size,
                 pretrained_concept_emb=None, freeze_ent_emb=True,
                 init_range=0.02, dropout=0.1):
        super().__init__()
        
        self.args = args
        
        # Load DRAGON backbone (encoder + GNN)
        self.dragon = DRAGON(
            args=args,
            k=k,
            n_ntype=n_ntype,
            n_etype=n_etype,
            sent_dim=sent_dim,
            n_concept=n_concept,
            concept_dim=concept_dim,
            concept_in_dim=concept_in_dim,
            hidden_size=hidden_size,
            pretrained_concept_emb=pretrained_concept_emb,
            freeze_ent_emb=freeze_ent_emb,
            init_range=init_range
        )
        
        # Binary classification head
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, 2)  # Binary: [0, 1]
        
        # Initialize classifier weights
        self.classifier.weight.data.normal_(mean=0.0, std=init_range)
        self.classifier.bias.data.zero_()
    
    def forward(self, *inputs, layer_id=-1, cache_output=False, detail=False):
        """
        Forward pass cho binary classification.
        
        Args:
            *inputs: C√°c inputs gi·ªëng nh∆∞ DRAGON g·ªëc:
                - sent_vecs: Token embeddings t·ª´ encoder
                - concept_ids: IDs c·ªßa concepts trong graph
                - node_type_ids: Types c·ªßa nodes
                - adj: Adjacency matrix c·ªßa graph
                - ...
            layer_id: Layer ƒë·ªÉ extract representation (default: -1 = last layer)
            cache_output: C√≥ cache intermediate outputs kh√¥ng
            detail: C√≥ return chi ti·∫øt kh√¥ng
            
        Returns:
            logits: Tensor [batch_size, 2] - logits cho 2 classes
            (optional) dragon_outputs: Outputs t·ª´ DRAGON backbone n·∫øu detail=True
        """
        
        # Get representation t·ª´ DRAGON
        # DRAGON tr·∫£ v·ªÅ: (logits_for_qa, hidden_states, ...)
        # Nh∆∞ng ta ch·ªâ c·∫ßn hidden states (pooled representation)
        dragon_outputs = self.dragon(*inputs, layer_id=layer_id, 
                                     cache_output=cache_output, detail=True)
        
        # Extract pooled representation
        # DRAGON's output structure: (logits, hidden_states, ...)
        # hidden_states shape: [batch_size, hidden_size]
        if isinstance(dragon_outputs, tuple):
            pooled_output = dragon_outputs[1]  # hidden_states
        else:
            pooled_output = dragon_outputs
        
        # Apply dropout v√† classifier
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)  # [batch_size, 2]
        
        if detail:
            return logits, dragon_outputs
        else:
            return logits
    
    def load_pretrained_dragon(self, model_path):
        """
        Load pre-trained DRAGON weights (t·ª´ general_model.pt).
        
        Args:
            model_path: Path ƒë·∫øn pre-trained checkpoint
        """
        print(f"üî• Loading pre-trained DRAGON from {model_path}")
        
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # Extract DRAGON weights (b·ªè qua classification head c≈©)
        dragon_state_dict = {}
        for key, value in checkpoint.items():
            # Ch·ªâ load encoder + GNN, b·ªè qua classifier c≈©
            if not key.startswith('classifier'):
                # Remove 'dragon.' prefix if exists
                new_key = key.replace('dragon.', '')
                dragon_state_dict[new_key] = value
        
        # Load v√†o DRAGON backbone
        missing_keys, unexpected_keys = self.dragon.load_state_dict(
            dragon_state_dict, strict=False
        )
        
        if missing_keys:
            print(f"‚ö†Ô∏è  Missing keys (expected for new classifier): {missing_keys[:5]}...")
        if unexpected_keys:
            print(f"‚ö†Ô∏è  Unexpected keys: {unexpected_keys[:5]}...")
        
        print("‚úÖ Pre-trained DRAGON loaded successfully!")
        print("üéØ Binary classifier head initialized randomly (will be fine-tuned)")
    
    def freeze_encoder(self):
        """Freeze DRAGON encoder (ch·ªâ train classifier head)."""
        for param in self.dragon.encoder.parameters():
            param.requires_grad = False
        print("‚ùÑÔ∏è  DRAGON encoder frozen")
    
    def unfreeze_encoder(self):
        """Unfreeze DRAGON encoder (fine-tune to√†n b·ªô)."""
        for param in self.dragon.encoder.parameters():
            param.requires_grad = True
        print("üî• DRAGON encoder unfrozen")
    
    def freeze_gnn(self):
        """Freeze GNN layers."""
        for param in self.dragon.gnn.parameters():
            param.requires_grad = False
        print("‚ùÑÔ∏è  GNN frozen")
    
    def unfreeze_gnn(self):
        """Unfreeze GNN layers."""
        for param in self.dragon.gnn.parameters():
            param.requires_grad = True
        print("üî• GNN unfrozen")


class DRAGONBinaryDataLoader(nn.Module):
    """
    DataLoader cho Binary Classification v·ªõi DRAGON.
    
    NOTE: Th·ª±c t·∫ø kh√¥ng c·∫ßn class n√†y v√¨ data_utils.DRAGON_DataLoader ƒë√£ support binary.
    Gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code docs.
    """
    pass


def create_optimizer_grouped_parameters(model, args):
    """
    T·∫°o optimizer v·ªõi learning rates kh√°c nhau cho c√°c parts.
    
    Strategy:
        - Encoder: Lower LR (2e-5) - ƒë√£ pre-trained
        - GNN: Medium LR (1e-3) - ƒë√£ pre-trained
        - Classifier: Higher LR (1e-3) - random init
    
    Args:
        model: DRAGONBinaryClassifier instance
        args: Arguments v·ªõi learning rates
    
    Returns:
        List of parameter groups cho optimizer
    """
    
    no_decay = ['bias', 'LayerNorm.weight']
    
    optimizer_grouped_parameters = [
        # Encoder v·ªõi weight decay
        {
            'params': [p for n, p in model.dragon.encoder.named_parameters()
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': args.weight_decay,
            'lr': args.encoder_lr
        },
        # Encoder kh√¥ng weight decay
        {
            'params': [p for n, p in model.dragon.encoder.named_parameters()
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
            'lr': args.encoder_lr
        },
        # GNN
        {
            'params': model.dragon.gnn.parameters(),
            'weight_decay': args.weight_decay,
            'lr': args.decoder_lr
        },
        # Classifier head (higher LR v√¨ random init)
        {
            'params': model.classifier.parameters(),
            'weight_decay': args.weight_decay,
            'lr': args.decoder_lr
        }
    ]
    
    return optimizer_grouped_parameters


# ============================================================================
# Usage Example
# ============================================================================

if __name__ == "__main__":
    """
    Example usage c·ªßa DRAGONBinaryClassifier.
    """
    
    print("üêâ DRAGON Binary Classification Example")
    print("=" * 60)
    
    # Gi·∫£ l·∫≠p args
    class Args:
        encoder = 'roberta-large'
        decoder = 'gnn'
        k = 5
        n_ntype = 4
        n_etype = 38
        concept_dim = 200
        dropout = 0.1
        encoder_lr = 2e-5
        decoder_lr = 1e-3
        weight_decay = 0.01
    
    args = Args()
    
    # Model config
    hidden_size = 1024  # RoBERTa-large
    n_concept = 799273  # ConceptNet
    
    print("\n1Ô∏è‚É£  Kh·ªüi t·∫°o model...")
    model = DRAGONBinaryClassifier(
        args=args,
        k=args.k,
        n_ntype=args.n_ntype,
        n_etype=args.n_etype,
        sent_dim=hidden_size,
        n_concept=n_concept,
        concept_dim=args.concept_dim,
        concept_in_dim=args.concept_dim,
        hidden_size=hidden_size,
        pretrained_concept_emb=None,
        freeze_ent_emb=True,
        dropout=args.dropout
    )
    print(f"‚úÖ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters")
    
    print("\n2Ô∏è‚É£  Load pre-trained weights...")
    # model.load_pretrained_dragon('models/general_model.pt')
    print("   (B·ªè qua v√¨ ch∆∞a c√≥ file)")
    
    print("\n3Ô∏è‚É£  T·∫°o optimizer v·ªõi grouped parameters...")
    param_groups = create_optimizer_grouped_parameters(model, args)
    print(f"‚úÖ Created {len(param_groups)} parameter groups")
    
    print("\n4Ô∏è‚É£  Test forward pass...")
    batch_size = 2
    seq_len = 128
    n_nodes = 50
    
    # Dummy inputs
    sent_vecs = torch.randn(batch_size, seq_len, hidden_size)
    concept_ids = torch.randint(0, n_concept, (batch_size, n_nodes))
    node_type_ids = torch.randint(0, args.n_ntype, (batch_size, n_nodes))
    adj = torch.randn(batch_size, args.n_etype, n_nodes, n_nodes)
    
    # Forward
    try:
        logits = model(sent_vecs, concept_ids, node_type_ids, adj)
        print(f"‚úÖ Forward pass successful!")
        print(f"   Input shape: {sent_vecs.shape}")
        print(f"   Output shape: {logits.shape}")
        print(f"   Output: {logits}")
    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
    
    print("\n" + "=" * 60)
    print("üéâ Example completed!")
    print("\nüí° Next steps:")
    print("   1. Integrate v√†o dragon.py training loop")
    print("   2. Update data preprocessing cho binary labels")
    print("   3. Create training script: run_train__isarcasm_binary.sh")
    print("   4. Fine-tune v√† enjoy the results! üöÄ")
